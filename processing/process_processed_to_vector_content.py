import json
from pathlib import Path
from collections import defaultdict
import re


def extract_table_captions(table_data):
    """
    table_dataì—ì„œ í…Œì´ë¸” ìº¡ì…˜/ì„¤ëª…ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    table_dataëŠ” ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:
    - dict: {'page_id': str, 'value': str} í˜•íƒœ
    - list: dictë“¤ì˜ ë¦¬ìŠ¤íŠ¸
    - str: ì´ë¯¸ íŒŒì‹±ëœ JSON ë¬¸ìì—´
    """
    if not table_data:
        return ""
    
    captions = []
    
    # table_dataê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
    if isinstance(table_data, list):
        for item in table_data:
            if isinstance(item, dict):
                # 'value' í•„ë“œì—ì„œ í…Œì´ë¸” ì„¤ëª… ì¶”ì¶œ
                value = item.get('value', '')
                if value:
                    captions.extend(_extract_descriptions_from_table_value(value))
            elif isinstance(item, str):
                # JSON ë¬¸ìì—´ì¸ ê²½ìš° íŒŒì‹± ì‹œë„
                try:
                    obj = json.loads(item)
                    if isinstance(obj, dict) and 'value' in obj:
                        captions.extend(_extract_descriptions_from_table_value(obj['value']))
                except Exception:
                    continue
    
    # table_dataê°€ dictì¸ ê²½ìš°
    elif isinstance(table_data, dict):
        value = table_data.get('value', '')
        if value:
            captions.extend(_extract_descriptions_from_table_value(value))
    
    # table_dataê°€ ë¬¸ìì—´ì¸ ê²½ìš° (JSON ë¬¸ìì—´ ë˜ëŠ” ì§ì ‘ í…Œì´ë¸” ë°ì´í„°)
    elif isinstance(table_data, str):
        try:
            obj = json.loads(table_data)
            if isinstance(obj, dict) and 'value' in obj:
                captions.extend(_extract_descriptions_from_table_value(obj['value']))
        except Exception:
            # JSONì´ ì•„ë‹Œ ê²½ìš° ì§ì ‘ ì²˜ë¦¬ (í…Œì´ë¸” ë°ì´í„° ë¬¸ìì—´)
            captions.extend(_extract_descriptions_from_table_value(table_data))
    
    # ì¤‘ë³µ ì œê±°í•˜ê³  ë°˜í™˜
    unique_captions = []
    seen = set()
    for caption in captions:
        if caption and caption not in seen:
            unique_captions.append(caption)
            seen.add(caption)
    
    return "; ".join(unique_captions)


def _extract_descriptions_from_table_value(value: str) -> list:
    """
    í…Œì´ë¸” value ë¬¸ìì—´ì—ì„œ ì„¤ëª… ë¶€ë¶„ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    "ì´ í‘œëŠ” ... ì¡°í•­ì— ëŒ€í•œ ìƒì„¸ë‚´ì—­ì„." í˜•íƒœì˜ ì„¤ëª…ì„ ì°¾ìŠµë‹ˆë‹¤.
    """
    descriptions = []
    if not value:
        return descriptions
    
    lines = value.split('\n')
    for line in lines:
        line = line.strip()
        # "ì´ í‘œëŠ”"ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì„¤ëª… ë¼ì¸ ì°¾ê¸°
        if line.startswith('ì´ í‘œëŠ”') or (line.startswith('"ì´ í‘œëŠ”') and '"ì´ í‘œëŠ”' in line):
            # ë”°ì˜´í‘œ ì œê±°
            line = line.strip('"').strip("'")
            # "ì¡°í•­ì— ëŒ€í•œ ìƒì„¸ë‚´ì—­" ë˜ëŠ” ìœ ì‚¬í•œ íŒ¨í„´ìœ¼ë¡œ ëë‚˜ëŠ”ì§€ í™•ì¸
            if 'ìƒì„¸ë‚´ì—­' in line or 'ì¡°í•­' in line:
                # ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±° (CSV í˜•ì‹ì˜ ë‚˜ë¨¸ì§€ ì»¬ëŸ¼ ì œê±°)
                description = line.split(',')[0].strip('"').strip()
                if description:
                    descriptions.append(description)
    
    return descriptions


def make_vector_content(merged_obj):
    """
    ê²€ìƒ‰ìš© ë²¡í„° ì½˜í…ì¸ ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        merged_obj: ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ê°€ ë³‘í•©ëœ dict ê°ì²´
            - 'title' ë˜ëŠ” 'id': ì œëª©
            - 'content': LLM ì‘ë‹µ dict (ë‚´ë¶€ì— 'summary' í¬í•¨)
            - 'toc': ë¬¸ì„œ êµ¬ì¡° ë¬¸ìì—´
            - 'value': í…Œì´ë¸” ë°ì´í„° ë¬¸ìì—´
    """
    # ì œëª© ì¶”ì¶œ
    title = merged_obj.get('title', '')
    
    # ë¬¸ì„œ ê°œìš” ì¶”ì¶œ (LLM ì‘ë‹µì˜ content.summary)
    summary = ""
    content = merged_obj.get('content', {})
    if isinstance(content, dict):
        summary = content.get('summary', '')
    
    # ë¬¸ì„œ êµ¬ì¡°(TOC) ì¶”ì¶œ
    toc_str = merged_obj.get('toc', '')
    
    # í…Œì´ë¸” ìº¡ì…˜ ì¶”ì¶œ
    table_data = merged_obj.get('value', '')
    table_captions = extract_table_captions(table_data)
    
    # ì„¹ì…˜ êµ¬ì„±
    sections = [
        f"ì œëª©: {title}",
        f"ë¬¸ì„œ ê°œìš”: {summary}",
        f"ë¬¸ì„œ êµ¬ì¡°: {toc_str}",
        f"í¬í•¨ëœ í‘œ ë‚´ìš©: {table_captions}"
    ]
    
    # ë¹ˆ ì„¹ì…˜ ì œê±°í•˜ê³  ì¡°ì¸
    non_empty_sections = [s for s in sections if s.split(':', 1)[-1].strip()]
    
    return "\n".join(non_empty_sections)

def get_obj_from_jsonl(file_path):
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = json.loads(line)

            data.append(line)
    return data

def get_obj_from_dir(dir_path):
    data = []
    for file in dir_path.iterdir():
        with open(file, 'r', encoding='utf-8') as f:
            data.append(json.load(f))
    return data

def extract_page_id_from_basename(basename):
    match = re.search(r'page_(\d+)_body_text', basename)
    if match:
        return int(match.group(1))
    else:
        raise ValueError(f"Invalid basename: {basename}")

def extract_page_ids_from_dir(dir_path):
    page_ids = []
    for file in dir_path.iterdir():
        page_id = extract_page_id_from_basename(file.stem)
        page_ids.append(page_id)
    
    return page_ids

def extract_page_ids_from_jsonl(file_path):
    page_ids = set()
    
    with open(file_path,'r', encoding='utf-8') as f:
        for line in f:
            try:
                line = json.loads(line)
            except json.JSONDecodeError:
                print(f"Skipping invalid JSON line: {line.strip()}")

            if isinstance(line, dict):
                page_id = line.get('page_id')
                if page_id and isinstance(page_id, str):
                    page_id = int(line.get('page_id'))
                    print(page_id)
                    page_ids.add(page_id)

    return page_ids

def main():
    # data sources
    merged_tables_path = Path('/Users/sychoi/projects/ProjectInsightHub/data/processed/merged_tables.jsonl')
    toc_path = Path('/Users/sychoi/projects/ProjectInsightHub/data/processed/html_body_toc.jsonl')
    merged_metadata_path = Path('/Users/sychoi/projects/ProjectInsightHub/data/processed/merged_metadata.jsonl')
    merged_llm_resps_path = Path('/Users/sychoi/projects/ProjectInsightHub/data/processed/merged_llm_resps.jsonl')

    table_data = get_obj_from_jsonl(merged_tables_path)
    toc = get_obj_from_jsonl(toc_path)
    metadata = get_obj_from_jsonl(merged_metadata_path)
    llm_responses = get_obj_from_jsonl(merged_llm_resps_path)

    data_sources = {
        "metadata": metadata,
        "toc": toc,
        "table_data": table_data,
        "llm_responses": llm_responses
    }

    main_map = defaultdict(dict)
    presence_check = defaultdict(set)

    for src_name, data in data_sources.items():
        for obj in data:
            pid = str(obj.get('page_id') or obj.get('id'))
            if not pid:
                continue

            main_map[pid].update(obj)
            # # ì˜ˆì‹œ: defaultdict(<class 'dict'>, {'1111': {'summary': 'ë³¸ ê³„ì•½ì„œëŠ” ...'}})
            presence_check[pid].add(src_name)
            # # ì˜ˆì‹œ: defaultdict(<class 'set'>, {'1111': {'metadata', 'table_data', ...}})

    final_data_sources = []
    incomplete_objs = []
    src_names_set = set(data_sources.keys())
    print(f'src_names_set: {src_names_set}')

    for pid, found_sources in presence_check.items():
        missing = src_names_set - found_sources

        if not missing:
            final_data_sources.append(main_map[pid])
        else:
            incomplete_obj = main_map[pid]
            incomplete_obj['missing_sources'] = list(missing)
            incomplete_objs.append(incomplete_obj)

    # ê²°ê³¼ ë¦¬í¬íŠ¸
    print(f"âœ… ì™„ì „í•œ ê°ì²´ (RAG Ready): {len(final_data_sources)}ê°œ")
    print(f"âš ï¸ ëˆ„ë½ ë°œìƒ ê°ì²´: {len(incomplete_objs)}ê°œ")

    # ëˆ„ë½ ìƒì„¸ í™•ì¸ (ì˜ˆì‹œ)
    if incomplete_objs:
        print(f"ì²« ë²ˆì§¸ ëˆ„ë½ ì˜ˆì‹œ (ID: {incomplete_objs[0].get('page_id') or incomplete_objs[0].get('id')}): {incomplete_objs[0]['missing_sources']} ì†ŒìŠ¤ ì—†ìŒ")
    
    # RAGë¥¼ ìœ„í•œ vector content ìƒì„±
    output_path = Path('/Users/sychoi/projects/ProjectInsightHub/data/processed/vector_contents.jsonl')
    vector_contents = []
    
    print("\nğŸ“ Vector content ìƒì„± ì¤‘...")
    for merged_obj in final_data_sources:
        page_id = str(merged_obj.get('page_id') or merged_obj.get('id', ''))
        vector_content = make_vector_content(merged_obj)
        
        vector_contents.append({
            'page_id': page_id,
            'vector_content': vector_content,
            'metadata': {
                'title': merged_obj.get('title', '')
            }
        })
    
    # JSONL íŒŒì¼ë¡œ ì €ì¥
    with open(output_path, 'w', encoding='utf-8') as f:
        for item in vector_contents:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"âœ… Vector content ìƒì„± ì™„ë£Œ: {len(vector_contents)}ê°œ")
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_path}")
    
    # ì²« ë²ˆì§¸ ì˜ˆì‹œ ì¶œë ¥
    if vector_contents:
        print("\nğŸ“„ ì²« ë²ˆì§¸ Vector Content ì˜ˆì‹œ:")
        print(f"Page ID: {vector_contents[0]['page_id']}")
        print(f"Title: {vector_contents[0]['metadata']['title']}")
        print(f"\n{vector_contents[0]['vector_content']}")
        print("-" * 80)


if __name__ == '__main__':
    main()

    
